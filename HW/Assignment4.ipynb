
Assignment4.ipynb
Assignment4.ipynb_
COSE461 Assignment 4: Neural Machine Translation
To get started, make a copy of the assignment by clicking File->Save a copy in drive... or 파일->드라이브에 사본 저장. You will need to be logged into a Google account.

In this assignment, we are going to implement Cherokee-to-English Neural Machine Translator based on RNNs. If you want to know more about the task itself, take a look at Zhang et al (2020).

1. Neural Machine Translation with RNNs (30 points)
In Machine Translation, our goal is to convert a sentence from the source language (e.g. Cherokee) to the target language (e.g. English). In this assignment, we will implement a sequence-to-sequence (Seq2Seq) network with attention, to build a Neural Machine Translation (NMT) system. In this section, we describe the training procedure for the proposed NMT system, which uses a Bidirectional LSTM Encoder and a Unidirectional LSTM Decoder.

rnn architecture.PNG

Figure 1: Seq2Seq Model with Multiplicative Attention, shown on the third step of the decoder.

Model description (training procedure)
Given a sentence in the source language, we look up the subword embeddings from an embeddings matrix, yielding x1,...,xm(xi∈Re×1), where m is the length of the source sentence and e is the embedding size. We feed these embeddings to the bidirectional encoder, yielding hidden states and cell states for both the forwards (→) and backwards (←) LSTMs. The forwards and backwards versions are concatenated to give hidden states henci and cell states cenci:
hencicenci=[henci←−−;henci−→−]  where  henci∈R2h×1,henci←−−,henci−→−∈Rh×1=[cenci←−−;cenci−→−]  where  cenci∈R2h×1,cenci←−−,cenci−→−∈Rh×11≤i≤m1≤i≤m

We then initialize the decoder's first hidden state hdec0 and cell state cdec0 with a linear projection of the encoder's final hidden state and final cell state.
hdec0cdec0=Wh[hencm←−−;hencm−→−]  where  hdec0∈Rh×1,Wh∈Rh×2h=Wc[cencm←−−;cencm−→−]  where  cdec0∈Rh×1,Wc∈Rh×2h

With the decoder initialized, we must now feed it a target sentence. On the tth step, we look up the embedding for the tth subword, yt∈Re×1. We then concatenate yt with the combined-output vector ot−1∈Rh×1 from the previous timestep (we will explain what this is later down) to produce y¯t∈R(e+h)×1. Note that for the first target subword (i.e. the start token) o0 is a zero-vector. We then feed y¯t as input to the decoder.

hdect,cdect=Decoder(y¯t,cdect−1,cdect−1)  where  hdect,cdect∈Rh×1

We then use hdect to compute multiplicative attention over henc1,...,hencm:
et,i=(hdect)⊤WattProjhenci  αt=softmax(et)  at=∑i=1mαt,ihenci  where  et∈Rm×1,WattProj∈Rh×2hwhere  α∈Rm×1where  at∈R2h×11≤i≤m

We now concatenate the attention output at with the decoder hidden state hdect and pass this through a linear layer, tanh, and dropout to attain the conbined-output vector ot.
ut=[at;hdect]  vt=Wuut  ot=dropout(tanh(vt))  where  ut∈R3h×1where  vt∈Rh×1,Wu∈Rh×3hwhere  ot∈Rh×1

Then, we produce a probability distribution Pt over target subwords at the tth timestep:
Pt=softmax(Wvocabot)  where  Pt∈RVt×1,Wvocab∈RVt×h

Here, Vt is the size of the target vocabulary. Finally, to train the network we then compute the softmax cross entropy loss between Pt and gt, where gt is the one-hot vector of the target subword at timestep t:
Jt(θ)=CrossEntropy(Pt,gt)
Here, θ represents all the parameters of the moddel and Jt(θ) is the loss on step t of the decoder. Now that we have described the model, let's try implementing it for Cherokee to English translation!

Run below once to download and import what you need. If you are interested in all the details, you can checkout the github repository itself to check other codes that are used throughout the assignment.


[ ]
!rm COSE461_a4_utils -rf
!git clone https://github.com/ku-dmlab/COSE461_a4_utils.git
!pip install sentencepiece
!pip install sacrebleu
import sys
UTILS_DIR = "/content/COSE461_a4_utils"
sys.path.append(UTILS_DIR)

import math

import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
import nltk
nltk.download('punkt')
from typing import List, Tuple, Dict, Set, Union
Cloning into 'COSE461_a4_utils'...
remote: Enumerating objects: 78, done.
remote: Counting objects: 100% (78/78), done.
remote: Compressing objects: 100% (32/32), done.
remote: Total 78 (delta 45), reused 78 (delta 45), pack-reused 0 (from 0)
Receiving objects: 100% (78/78), 1.34 MiB | 2.86 MiB/s, done.
Resolving deltas: 100% (45/45), done.
Requirement already satisfied: sentencepiece in /usr/local/lib/python3.11/dist-packages (0.2.0)
Collecting sacrebleu
  Downloading sacrebleu-2.5.1-py3-none-any.whl.metadata (51 kB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 51.8/51.8 kB 2.7 MB/s eta 0:00:00
Collecting portalocker (from sacrebleu)
  Downloading portalocker-3.1.1-py3-none-any.whl.metadata (8.6 kB)
Requirement already satisfied: regex in /usr/local/lib/python3.11/dist-packages (from sacrebleu) (2024.11.6)
Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.11/dist-packages (from sacrebleu) (0.9.0)
Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from sacrebleu) (2.0.2)
Collecting colorama (from sacrebleu)
  Downloading colorama-0.4.6-py2.py3-none-any.whl.metadata (17 kB)
Requirement already satisfied: lxml in /usr/local/lib/python3.11/dist-packages (from sacrebleu) (5.3.2)
Downloading sacrebleu-2.5.1-py3-none-any.whl (104 kB)
   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 104.1/104.1 kB 6.0 MB/s eta 0:00:00
Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)
Downloading portalocker-3.1.1-py3-none-any.whl (19 kB)
Installing collected packages: portalocker, colorama, sacrebleu
Successfully installed colorama-0.4.6 portalocker-3.1.1 sacrebleu-2.5.1
[nltk_data] Downloading package punkt to /root/nltk_data...
[nltk_data]   Unzipping tokenizers/punkt.zip.
Question 1.1 [coding] (2 points)
In order to apply tensor operations, we must ensure that the sentences in a given batch are of the same length. Thus, we must identify the longest sentence in a batch and pad others to be the same length. Implement the pad_sents function in the following, which shall produce these padded sentences.


[ ]
def pad_sents(sents, pad_token):
    """ Pad list of sentences according to the longest sentence in the batch.
        The paddings should be at the end of each sentence.
    @param sents (list[list[str]]): list of sentences, where each sentence
                                    is represented as a list of words
    @param pad_token (str): padding token
    @returns sents_padded (list[list[str]]): list of sentences where sentences shorter
        than the max length sentence are padded out with the pad_token, such that
        each sentences in the batch now has equal length.
    """
    sents_padded = []

    ### YOUR CODE HERE (~6 Lines)
    max_len = max(len(s) for s in sents)
    for s in sents:
        padded = s + [pad_token] * (max_len - len(s))
        sents_padded.append(padded)
    ### END YOUR CODE

    return sents_padded

# Vocab class uses this implementation. rerun vocab class to take your
# implementation into account
import vocab
execfile(vocab.__file__)
Question 1.2 [coding] (3 points)
Implement the __init__ function of following ModelEmbeddings class to initialize the necessary source and target embeddings.


[ ]
class ModelEmbeddings(nn.Module):
    """
    Class that converts input words to their embeddings.
    """
    def __init__(self, embed_size, vocab):
        """
        Init the Embedding layers.

        @param embed_size (int): Embedding size (dimensionality)
        @param vocab (Vocab): Vocabulary object containing src and tgt languages
                              See vocab.py for documentation.
        """
        super(ModelEmbeddings, self).__init__()
        self.embed_size = embed_size

        # default values
        self.source = None
        self.target = None

        src_pad_token_idx = vocab.src['<pad>']
        tgt_pad_token_idx = vocab.tgt['<pad>']

        ### YOUR CODE HERE (~2 Lines)
        ### TODO - Initialize the following variables:
        ###     self.source (Embedding Layer for source language)
        ###     self.target (Embedding Layer for target langauge)
        ###
        ### Note:
        ###     1. `vocab` object contains two vocabularies:
        ###            `vocab.src` for source
        ###            `vocab.tgt` for target
        ###     2. You can get the length of a specific vocabulary by running:
        ###             `len(vocab.<specific_vocabulary>)`
        ###     3. Remember to include the padding token for the specific vocabulary
        ###        when creating your Embedding.
        ###
        ### Use the following docs to properly initialize these variables:
        ###     Embedding Layer:
        ###         https://pytorch.org/docs/stable/nn.html#torch.nn.Embedding
        self.source = nn.Embedding(len(vocab.src), embed_size, padding_idx=src_pad_token_idx)
        self.target = nn.Embedding(len(vocab.tgt), embed_size, padding_idx=tgt_pad_token_idx)
        ### END YOUR CODE

Question 1.3 [coding] (3 points)
Implement the __init__ function of following NMT class to initialize the necessary model embeddings (using the ModelEmbeddings class) and layers (LSTM, projection, and dropout) for the NMT system.


[ ]
class NMT(nn.Module):
    """ Simple Neural Machine Translation Model:
        - Bidrectional LSTM Encoder
        - Unidirection LSTM Decoder
        - Global Attention Model (Luong, et al. 2015)
    """
    def __init__(self, embed_size, hidden_size, vocab, dropout_rate=0.2):
        """ Init NMT Model.

        @param embed_size (int): Embedding size (dimensionality)
        @param hidden_size (int): Hidden Size, the size of hidden states (dimensionality)
        @param vocab (Vocab): Vocabulary object containing src and tgt languages
                              See vocab.py for documentation.
        @param dropout_rate (float): Dropout probability, for attention
        """
        super(NMT, self).__init__()
        self.model_embeddings = ModelEmbeddings(embed_size, vocab)
        self.hidden_size = hidden_size
        self.dropout_rate = dropout_rate
        self.vocab = vocab

        # default values
        self.encoder = None
        self.decoder = None
        self.h_projection = None
        self.c_projection = None
        self.att_projection = None
        self.combined_output_projection = None
        self.target_vocab_projection = None
        self.dropout = None
        # For sanity check only, not relevant to implementation
        self.gen_sanity_check = False
        self.counter = 0


        ### YOUR CODE HERE (~8 Lines)
        ### TODO - Initialize the following variables:
        ###     self.encoder (Bidirectional LSTM with bias)
        ###     self.decoder (LSTM Cell with bias)
        ###     self.h_projection (Linear Layer with no bias), called W_{h} in the equations.
        ###     self.c_projection (Linear Layer with no bias), called W_{c} in the equations.
        ###     self.att_projection (Linear Layer with no bias), called W_{attProj} in the equations.
        ###     self.combined_output_projection (Linear Layer with no bias), called W_{u} in the equations.
        ###     self.target_vocab_projection (Linear Layer with no bias), called W_{vocab} in the equations.
        ###     self.dropout (Dropout Layer with dropout_rate)
        ###
        ### Use the following docs to properly initialize these variables:
        ###     LSTM:
        ###         https://pytorch.org/docs/stable/nn.html#torch.nn.LSTM
        ###     LSTM Cell:
        ###         https://pytorch.org/docs/stable/nn.html#torch.nn.LSTMCell
        ###     Linear Layer:
        ###         https://pytorch.org/docs/stable/nn.html#torch.nn.Linear
        ###     Dropout Layer:
        ###         https://pytorch.org/docs/stable/nn.html#torch.nn.Dropout
        self.encoder = nn.LSTM(embed_size, hidden_size, bidirectional=True)
        self.decoder = nn.LSTMCell(embed_size + hidden_size, hidden_size)
        self.h_projection = nn.Linear(2 * hidden_size, hidden_size, bias=False)
        self.c_projection = nn.Linear(2 * hidden_size, hidden_size, bias=False)
        self.att_projection = nn.Linear(2 * hidden_size, hidden_size, bias=False)
        self.combined_output_projection = nn.Linear(3 * hidden_size, hidden_size, bias=False)
        self.target_vocab_projection = nn.Linear(hidden_size, len(vocab.tgt), bias=False)
        self.dropout = nn.Dropout(dropout_rate)

        ### END YOUR CODE


    def forward(self, source: List[List[str]], target: List[List[str]]) -> torch.Tensor:
        """ Take a mini-batch of source and target sentences, compute the log-likelihood of
        target sentences under the language models learned by the NMT system.

        @param source (List[List[str]]): list of source sentence tokens
        @param target (List[List[str]]): list of target sentence tokens, wrapped by `<s>` and `</s>`

        @returns scores (Tensor): a variable/tensor of shape (b, ) representing the
                                    log-likelihood of generating the gold-standard target sentence for
                                    each example in the input batch. Here b = batch size.
        """
        # Compute sentence lengths
        source_lengths = [len(s) for s in source]

        # Convert list of lists into tensors
        source_padded = self.vocab.src.to_input_tensor(source, device=self.device)   # Tensor: (src_len, b)
        target_padded = self.vocab.tgt.to_input_tensor(target, device=self.device)   # Tensor: (tgt_len, b)

        ###     Run the network forward:
        ###     1. Apply the encoder to `source_padded` by calling `encode()`
        ###     2. Generate sentence masks for `source_padded` by calling `generate_sent_masks()`
        ###     3. Apply the decoder to compute combined-output by calling `decode()`
        ###     4. Compute log probability distribution over the target vocabulary using the
        ###        combined_outputs returned by the `decode()` function.

        enc_hiddens, dec_init_state = encode(self, source_padded, source_lengths) # Implemented in Question 1.3
        enc_masks = self.generate_sent_masks(enc_hiddens, source_lengths)
        combined_outputs = decode(self, enc_hiddens, enc_masks, dec_init_state, target_padded)
        P = F.log_softmax(self.target_vocab_projection(combined_outputs), dim=-1)

        # Zero out, probabilities for which we have nothing in the target text
        target_masks = (target_padded != self.vocab.tgt['<pad>']).float()

        # Compute log probability of generating true target words
        target_gold_words_log_prob = torch.gather(P, index=target_padded[1:].unsqueeze(-1), dim=-1).squeeze(-1) * target_masks[1:]
        scores = target_gold_words_log_prob.sum(dim=0)
        return scores

    def generate_sent_masks(self, enc_hiddens: torch.Tensor, source_lengths: List[int]) -> torch.Tensor:
        """ Generate sentence masks for encoder hidden states.

        @param enc_hiddens (Tensor): encodings of shape (b, src_len, 2*h), where b = batch size,
                                      src_len = max source length, h = hidden size.
        @param source_lengths (List[int]): List of actual lengths for each of the sentences in the batch.

        @returns enc_masks (Tensor): Tensor of sentence masks of shape (b, src_len),
                                    where src_len = max source length, h = hidden size.
        """
        enc_masks = torch.zeros(enc_hiddens.size(0), enc_hiddens.size(1), dtype=torch.float)
        for e_id, src_len in enumerate(source_lengths):
            enc_masks[e_id, src_len:] = 1
        return enc_masks.to(self.device)

    @property
    def device(self) -> torch.device:
        """ Determine which device to place the Tensors upon, CPU or GPU.
        """
        return self.model_embeddings.source.weight.device

    @staticmethod
    def load(model_path: str):
        """ Load the model from a file.
        @param model_path (str): path to model
        """
        params = torch.load(model_path, map_location=lambda storage, loc: storage, weights_only=False)
        args = params['args']
        model = NMT(vocab=params['vocab'], **args)
        model.load_state_dict(params['state_dict'])

        return model

    def save(self, path: str):
        """ Save the odel to a file.
        @param path (str): path to the model
        """
        print('save model parameters to [%s]' % path, file=sys.stderr)

        params = {
            'args': dict(embed_size=self.model_embeddings.embed_size, hidden_size=self.hidden_size, dropout_rate=self.dropout_rate),
            'vocab': self.vocab,
            'state_dict': self.state_dict()
        }

        torch.save(params, path)
Question 1.4 [coding] (4 points)
Implement the following encode function. This function converts the padded source sentences into the tensor X, generates henc1,...,hencm, and computes the initial state hdec0 and initial cell cdec0 for the Decoder.


[ ]
from torch.nn.utils.rnn import pad_packed_sequence, pack_padded_sequence

def encode(model, source_padded: torch.Tensor, source_lengths: List[int]) -> Tuple[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]:
    """ Apply the encoder to source sentences to obtain encoder hidden states.
        Additionally, take the final states of the encoder and project them to obtain initial states for decoder.

    @param model (NMT): nn.Module you have implemented in question 1.3
    @param source_padded (Tensor): Tensor of padded source sentences with shape (src_len, b), where
                                    b = batch_size, src_len = maximum source sentence length. Note that
                                    these have already been sorted in order of longest to shortest sentence.
    @param source_lengths (List[int]): List of actual lengths for each of the source sentences in the batch
    @returns enc_hiddens (Tensor): Tensor of hidden units with shape (b, src_len, h*2), where
                                    b = batch size, src_len = maximum source sentence length, h = hidden size.
    @returns dec_init_state (tuple(Tensor, Tensor)): Tuple of tensors representing the decoder's initial
                                            hidden state and cell.
    """
    enc_hiddens, dec_init_state = None, None

    ### YOUR CODE HERE (~ 8 Lines)
    ### TODO:
    ###     1. Construct Tensor `X` of source sentences with shape (src_len, b, e) using the source model embeddings.
    ###         src_len = maximum source sentence length, b = batch size, e = embedding size. Note
    ###         that there is no initial hidden state or cell for the decoder.
    ###     2. Compute `enc_hiddens`, `last_hidden`, `last_cell` by applying the encoder to `X`.
    ###         - Before you can apply the encoder, you need to apply the `pack_padded_sequence` function to X.
    ###         - After you apply the encoder, you need to apply the `pad_packed_sequence` function to enc_hiddens.
    ###         - Note that the shape of the tensor returned by the encoder is (src_len, b, h*2) and we want to
    ###           return a tensor of shape (b, src_len, h*2) as `enc_hiddens`.
    ###     3. Compute `dec_init_state` = (init_decoder_hidden, init_decoder_cell):
    ###         - `init_decoder_hidden`:
    ###             `last_hidden` is a tensor shape (2, b, h). The first dimension corresponds to forwards and backwards.
    ###             Concatenate the forwards and backwards tensors to obtain a tensor shape (b, 2*h).
    ###             Apply the h_projection layer to this in order to compute init_decoder_hidden.
    ###             This is h_0^{dec} in the equations. Here b = batch size, h = hidden size
    ###         - `init_decoder_cell`:
    ###             `last_cell` is a tensor shape (2, b, h). The first dimension corresponds to forwards and backwards.
    ###             Concatenate the forwards and backwards tensors to obtain a tensor shape (b, 2*h).
    ###             Apply the c_projection layer to this in order to compute init_decoder_cell.
    ###             This is c_0^{dec} in the equations. Here b = batch size, h = hidden size
    ###
    ### See the following docs, as you may need to use some of the following functions in your implementation:
    ###     Pack the padded sequence X before passing to the encoder:
    ###         https://pytorch.org/docs/stable/nn.html#torch.nn.utils.rnn.pack_padded_sequence
    ###     Pad the packed sequence, enc_hiddens, returned by the encoder:
    ###         https://pytorch.org/docs/stable/nn.html#torch.nn.utils.rnn.pad_packed_sequence
    ###     Tensor Concatenation:
    ###         https://pytorch.org/docs/stable/torch.html#torch.cat
    ###     Tensor Permute:
    ###         https://pytorch.org/docs/stable/tensors.html#torch.Tensor.permute
    X = model.model_embeddings.source(source_padded)
    packed_X = pack_padded_sequence(X, source_lengths, enforce_sorted=False)
    enc_hiddens, (last_hidden, last_cell) = model.encoder(packed_X)
    enc_hiddens, _ = pad_packed_sequence(enc_hiddens)
    enc_hiddens = enc_hiddens.permute(1, 0, 2)
    last_hidden_cat = torch.cat([last_hidden[0], last_hidden[1]], dim=1)
    last_cell_cat = torch.cat([last_cell[0], last_cell[1]], dim=1)
    dec_init_state = ((model.h_projection(last_hidden_cat)), (model.c_projection(last_cell_cat)))

    ### END YOUR CODE

    return enc_hiddens, dec_init_state
You can run a non-comprehensive sanity check by executing the following:


[ ]
nltk.download('punkt_tab')
[nltk_data] Downloading package punkt_tab to /root/nltk_data...
[nltk_data]   Unzipping tokenizers/punkt_tab.zip.
True

[ ]
from sanity_check import question_1_4_sanity_check
question_1_4_sanity_check(NMT, Vocab, UTILS_DIR, encode)
[nltk_data] Downloading package punkt to /root/nltk_data...
[nltk_data]   Package punkt is already up-to-date!
Running Sanity Check for Question 1.4: Encode
--------------------------------------------------------------------------------
enc_hiddens Sanity Checks Passed!
dec_init_state[0] Sanity Checks Passed!
dec_init_state[1] Sanity Checks Passed!
--------------------------------------------------------------------------------
All Sanity Checks Passed for Question 1.4: Encode!
--------------------------------------------------------------------------------
Question 1.5 [coding] (4 points)
Implement the following decode function. This function constructs y¯ and runs the step function over every timestep for the input.


[ ]
def decode(model, enc_hiddens: torch.Tensor, enc_masks: torch.Tensor,
            dec_init_state: Tuple[torch.Tensor, torch.Tensor], target_padded: torch.Tensor) -> torch.Tensor:
    """Compute combined output vectors for a batch.

    @param model (NMT): nn.Module you have implemented in question 1.3
    @param enc_hiddens (Tensor): Hidden states (b, src_len, h*2), where
                                  b = batch size, src_len = maximum source sentence length, h = hidden size.
    @param enc_masks (Tensor): Tensor of sentence masks (b, src_len), where
                                  b = batch size, src_len = maximum source sentence length.
    @param dec_init_state (tuple(Tensor, Tensor)): Initial state and cell for decoder
    @param target_padded (Tensor): Gold-standard padded target sentences (tgt_len, b), where
                                    tgt_len = maximum target sentence length, b = batch size.

    @returns combined_outputs (Tensor): combined output tensor  (tgt_len, b,  h), where
                                    tgt_len = maximum target sentence length, b = batch_size,  h = hidden size
    """
    # Chop of the <END> token for max length sentences.
    target_padded = target_padded[:-1]

    # Initialize the decoder state (hidden and cell)
    dec_state = dec_init_state

    # Initialize previous combined output vector o_{t-1} as zero
    batch_size = enc_hiddens.size(0)
    o_prev = torch.zeros(batch_size, model.hidden_size, device=model.device)

    # Initialize a list we will use to collect the combined output o_t on each step
    combined_outputs = []

    ### YOUR CODE HERE (~9 Lines)
    ### TODO:
    ###     1. Apply the attention projection layer to `enc_hiddens` to obtain `enc_hiddens_proj`,
    ###         which should be shape (b, src_len, h),
    ###         where b = batch size, src_len = maximum source length, h = hidden size.
    ###         This is applying W_{attProj} to h^enc, as described in the equations.
    ###     2. Construct tensor `Y` of target sentences with shape (tgt_len, b, e) using the target model embeddings.
    ###         where tgt_len = maximum target sentence length, b = batch size, e = embedding size.
    ###     3. Use the torch.split function to iterate over the time dimension of Y.
    ###         Within the loop, this will give you Y_t of shape (1, b, e) where b = batch size, e = embedding size.
    ###             - Squeeze Y_t into a tensor of dimension (b, e).
    ###             - Construct Ybar_t by concatenating Y_t with o_prev on their last dimension
    ###             - Use the step function (will be implemented in question 1.6)
    ###               to compute the the Decoder's next (cell, state) values
    ###               as well as the new combined output o_t.
    ###             - Append o_t to combined_outputs
    ###             - Update o_prev to the new o_t.
    ###     4. Use torch.stack to convert combined_outputs from a list length tgt_len of
    ###         tensors shape (b, h), to a single tensor shape (tgt_len, b, h)
    ###         where tgt_len = maximum target sentence length, b = batch size, h = hidden size.
    ###
    ### Note:
    ###    - When using the squeeze() function make sure to specify the dimension you want to squeeze
    ###      over. Otherwise, you will remove the batch dimension accidentally, if batch_size = 1.
    ###
    ### You may find some of these functions useful:
    ###     Zeros Tensor:
    ###         https://pytorch.org/docs/stable/torch.html#torch.zeros
    ###     Tensor Splitting (iteration):
    ###         https://pytorch.org/docs/stable/torch.html#torch.split
    ###     Tensor Dimension Squeezing:
    ###         https://pytorch.org/docs/stable/torch.html#torch.squeeze
    ###     Tensor Concatenation:
    ###         https://pytorch.org/docs/stable/torch.html#torch.cat
    ###     Tensor Stacking:
    ###         https://pytorch.org/docs/stable/torch.html#torch.stack
    enc_hiddens_proj = model.att_projection(enc_hiddens)
    Y = model.model_embeddings.target(target_padded)
    for Y_t in torch.split(Y, 1, dim=0):
        Y_t = Y_t.squeeze(0)
        Ybar_t = torch.cat([Y_t, o_prev], dim=1)
        dec_state, o_t, _ = step(model, Ybar_t, dec_state, enc_hiddens, enc_hiddens_proj, enc_masks)
        combined_outputs.append(o_t)
        o_prev = o_t
    combined_outputs = torch.stack(combined_outputs)

    ### END YOUR CODE

    return combined_outputs
You can run a non-comprehensive sanity check by executing the following - however, it requires implementing step function, so come back after solving Question 1.6.


[ ]
from sanity_check import question_1_5_sanity_check
question_1_5_sanity_check(NMT, Vocab, UTILS_DIR, decode)
--------------------------------------------------------------------------------
Running Sanity Check for Question 1.5: Decode
--------------------------------------------------------------------------------
torch.Size([23, 5, 3])
combined_outputs Sanity Checks Passed!
--------------------------------------------------------------------------------
All Sanity Checks Passed for Question 1.5: Decode!
--------------------------------------------------------------------------------
Question 1.6 [coding] (4 points)
Implement the following step function. This function applies the Decoder's LSTM cell for a single timestep, computing the encoding of the target subword hdect, the attention scores et, attention distribution αt, the attention output at, and finally the combined output ot.


[ ]
def step(model, Ybar_t: torch.Tensor,
            dec_state: Tuple[torch.Tensor, torch.Tensor],
            enc_hiddens: torch.Tensor,
            enc_hiddens_proj: torch.Tensor,
            enc_masks: torch.Tensor) -> Tuple[Tuple, torch.Tensor, torch.Tensor]:
    """ Compute one forward step of the LSTM decoder, including the attention computation.

    @param Ybar_t (Tensor): Concatenated Tensor of [Y_t o_prev], with shape (b, e + h). The input for the decoder,
                            where b = batch size, e = embedding size, h = hidden size.
    @param dec_state (tuple(Tensor, Tensor)): Tuple of tensors both with shape (b, h), where b = batch size, h = hidden size.
            First tensor is decoder's prev hidden state, second tensor is decoder's prev cell.
    @param enc_hiddens (Tensor): Encoder hidden states Tensor, with shape (b, src_len, h * 2), where b = batch size,
                                src_len = maximum source length, h = hidden size.
    @param enc_hiddens_proj (Tensor): Encoder hidden states Tensor, projected from (h * 2) to h. Tensor is with shape (b, src_len, h),
                                where b = batch size, src_len = maximum source length, h = hidden size.
    @param enc_masks (Tensor): Tensor of sentence masks shape (b, src_len),
                                where b = batch size, src_len is maximum source length.

    @returns dec_state (tuple (Tensor, Tensor)): Tuple of tensors both shape (b, h), where b = batch size, h = hidden size.
            First tensor is decoder's new hidden state, second tensor is decoder's new cell.
    @returns combined_output (Tensor): Combined output Tensor at timestep t, shape (b, h), where b = batch size, h = hidden size.
    @returns e_t (Tensor): Tensor of shape (b, src_len). It is attention scores distribution.
                            Note: You will not use this outside of this function.
                                  We are simply returning this value so that we can sanity check
                                  your implementation.
    """

    combined_output = None

    ### YOUR CODE HERE (~3 Lines)
    ### TODO:
    ###     1. Apply the decoder to `Ybar_t` and `dec_state`to obtain the new dec_state.
    ###     2. Split dec_state into its two parts (dec_hidden, dec_cell)
    ###     3. Compute the attention scores e_t, a Tensor shape (b, src_len).
    ###        Note: b = batch_size, src_len = maximum source length, h = hidden size.
    ###
    ###       Hints:
    ###         - dec_hidden is shape (b, h) and corresponds to h^dec_t in the equations (batched)
    ###         - enc_hiddens_proj is shape (b, src_len, h) and corresponds to W_{attProj} h^enc (batched).
    ###         - Use batched matrix multiplication (torch.bmm) to compute e_t (be careful about the input/ output shapes!)
    ###         - To get the tensors into the right shapes for bmm, you will need to do some squeezing and unsqueezing.
    ###         - When using the squeeze() function make sure to specify the dimension you want to squeeze
    ###             over. Otherwise, you will remove the batch dimension accidentally, if batch_size = 1.
    ###
    ### Use the following docs to implement this functionality:
    ###     Batch Multiplication:
    ###        https://pytorch.org/docs/stable/torch.html#torch.bmm
    ###     Tensor Unsqueeze:
    ###         https://pytorch.org/docs/stable/torch.html#torch.unsqueeze
    ###     Tensor Squeeze:
    ###         https://pytorch.org/docs/stable/torch.html#torch.squeeze
    dec_hidden, dec_cell = model.decoder(Ybar_t, dec_state)
    dec_state = (dec_hidden, dec_cell)
    # Attention scores: (b, src_len)
    e_t = torch.bmm(enc_hiddens_proj, dec_hidden.unsqueeze(2)).squeeze(2)

    ### END YOUR CODE

    # Set e_t to -inf where enc_masks has 1
    if enc_masks is not None:
        e_t.data.masked_fill_(enc_masks.bool(), -float('inf'))

    ### YOUR CODE HERE (~6 Lines)
    ### TODO:
    ###     1. Apply softmax to e_t to yield alpha_t
    ###     2. Use batched matrix multiplication between alpha_t and enc_hiddens to obtain the
    ###         attention output vector, a_t.
    #$$     Hints:
    ###           - alpha_t is shape (b, src_len)
    ###           - enc_hiddens is shape (b, src_len, 2h)
    ###           - a_t should be shape (b, 2h)
    ###           - You will need to do some squeezing and unsqueezing.
    ###     Note: b = batch size, src_len = maximum source length, h = hidden size.
    ###
    ###     3. Concatenate dec_hidden with a_t to compute tensor U_t
    ###     4. Apply the combined output projection layer to U_t to compute tensor V_t
    ###     5. Compute tensor O_t by first applying the Tanh function and then the dropout layer.
    ###
    ### Use the following docs to implement this functionality:
    ###     Softmax:
    ###         https://pytorch.org/docs/stable/nn.html#torch.nn.functional.softmax
    ###     Batch Multiplication:
    ###        https://pytorch.org/docs/stable/torch.html#torch.bmm
    ###     Tensor View:
    ###         https://pytorch.org/docs/stable/tensors.html#torch.Tensor.view
    ###     Tensor Concatenation:
    ###         https://pytorch.org/docs/stable/torch.html#torch.cat
    ###     Tanh:
    ###         https://pytorch.org/docs/stable/torch.html#torch.tanh
    alpha_t = F.softmax(e_t, dim=1)
    a_t = torch.bmm(alpha_t.unsqueeze(1), enc_hiddens).squeeze(1)
    U_t = torch.cat([a_t, dec_hidden], dim=1)
    V_t = model.combined_output_projection(U_t)
    O_t = model.dropout(torch.tanh(V_t))

    ### END YOUR CODE

    combined_output = O_t
    return dec_state, combined_output, e_t
You can run a non-comprehensive sanity check by executing the following:


[ ]
from sanity_check import question_1_6_sanity_check
question_1_6_sanity_check(NMT, Vocab, UTILS_DIR, step)
--------------------------------------------------------------------------------
Running Sanity Check for Question 1.6: Step
--------------------------------------------------------------------------------
dec_state[0] Sanity Checks Passed!
dec_state[1] Sanity Checks Passed!
combined_output  Sanity Checks Passed!
e_t Sanity Checks Passed!
--------------------------------------------------------------------------------
All Sanity Checks Passed for Question 1.6: Step!
--------------------------------------------------------------------------------
Question 1.7 [written] (3 points)
Recall the generate_sent_masks() function in NMT of question 1.3. It produces a tensor called enc_masks. It has shape (batch size, max source sentence length) and contains 1s in positions corresponding to 'pad' tokens in the input, and 0s for non-pad tokens. Look at how the masks are used during the attention computation in the step() function.

First explain (in around three sentences) what effect the masks have on the entire attention computation. Then explain (in one or two sentences) why it is necessary to use the masks in this way.

The masks in the attention computation serve the purpose of masking out the padded tokens in the source sequence during the attention calculation. By setting the attention scores corresponding to the pad tokens to -inf (negative infinity) before applying the softmax, the masks ensure that these padded tokens are not attended to, effectively excluding them from the attention distribution.

Using the masks in this way is necessary because padded tokens do not carry any meaningful information and including them in the attention computation could lead to incorrect alignments. By masking out the padded tokens, the model focuses its attention only on the relevant source tokens, improving the quality of the attention mechanism and the overall translation performance.

-The masks ensure that the attention mechanism assigns zero probability to padded positions in the source sentence by masking out these positions before the softmax, so only valid source words are use when computing the context vector. This will prevents the model from using meaningless padding information during translation.

-Using masks in this way is necessary because it will make sure the model focuses only on relevant input tokens, leading to more accurate attention and better overall translation performance.

Now it's time to get things running!
By executing the following, you generate the necessary vocab file.


[ ]
get_vocab(f"{UTILS_DIR}/chr_en_data/train.chr", f"{UTILS_DIR}/chr_en_data/train.en", "vocab.json")
read in source sentences: /content/COSE461_a4_utils/chr_en_data/train.chr
read in target sentences: /content/COSE461_a4_utils/chr_en_data/train.en
initialize source vocabulary ..
initialize target vocabulary ..
generated vocabulary, source 21000 words, target 8000 words
vocabulary saved to vocab.json
By executing the following, you get train and test function that is needed to run your code. If you want to know more about the details, you can find out what is inside those funtions by going to this repository and checking run.py out.


[ ]
import run
from torch.serialization import safe_globals

# allow torch.load to unpickle Vocab class
torch.serialization.add_safe_globals([Vocab])
torch.serialization.add_safe_globals([VocabEntry])
execfile(run.__file__)
Now it is time to run the code! Note that running below requires GPU, so if you haven't set it, go to Edit->Notebook settings or 수정->노트 설정 and choose GPU in the Hardware accelerator dropdown. If you changed it here, you are required to reset the runtime and re-run all the codes above.

It should take under 1 hour.


[ ]
train(NMT, Vocab, UTILS_DIR)
uniformly initialize parameters [-0.100000, +0.100000]
use device: cuda:0
begin Maximum Likelihood training
epoch 1, iter 10, avg. loss 208.22, avg. ppl 3310.29 cum. examples 320, speed 2599.60 words/sec, time elapsed 3.16 sec
epoch 1, iter 20, avg. loss 177.55, avg. ppl 756.74 cum. examples 640, speed 2865.38 words/sec, time elapsed 6.15 sec
epoch 1, iter 30, avg. loss 164.98, avg. ppl 544.20 cum. examples 960, speed 3409.25 words/sec, time elapsed 8.61 sec
epoch 1, iter 40, avg. loss 170.42, avg. ppl 472.69 cum. examples 1280, speed 3394.03 words/sec, time elapsed 11.22 sec
epoch 1, iter 50, avg. loss 161.88, avg. ppl 367.69 cum. examples 1600, speed 3813.66 words/sec, time elapsed 13.52 sec
epoch 1, iter 60, avg. loss 153.79, avg. ppl 348.51 cum. examples 1920, speed 3626.16 words/sec, time elapsed 15.84 sec
epoch 1, iter 70, avg. loss 153.59, avg. ppl 302.72 cum. examples 2240, speed 3426.84 words/sec, time elapsed 18.35 sec
epoch 1, iter 80, avg. loss 151.06, avg. ppl 300.18 cum. examples 2560, speed 3416.94 words/sec, time elapsed 20.83 sec
epoch 1, iter 90, avg. loss 144.62, avg. ppl 271.93 cum. examples 2880, speed 3284.76 words/sec, time elapsed 23.34 sec
epoch 1, iter 100, avg. loss 143.10, avg. ppl 225.29 cum. examples 3200, speed 3680.37 words/sec, time elapsed 25.64 sec
epoch 1, iter 110, avg. loss 139.96, avg. ppl 225.46 cum. examples 3520, speed 3529.70 words/sec, time elapsed 27.98 sec
epoch 1, iter 120, avg. loss 129.51, avg. ppl 193.49 cum. examples 3840, speed 3715.71 words/sec, time elapsed 30.10 sec
epoch 1, iter 130, avg. loss 136.57, avg. ppl 182.86 cum. examples 4160, speed 3810.40 words/sec, time elapsed 32.30 sec
epoch 1, iter 140, avg. loss 133.22, avg. ppl 183.73 cum. examples 4480, speed 3594.47 words/sec, time elapsed 34.58 sec
epoch 1, iter 150, avg. loss 134.62, avg. ppl 171.43 cum. examples 4800, speed 3885.25 words/sec, time elapsed 36.73 sec
epoch 1, iter 160, avg. loss 129.94, avg. ppl 160.60 cum. examples 5120, speed 3593.11 words/sec, time elapsed 39.01 sec
epoch 1, iter 170, avg. loss 132.58, avg. ppl 165.11 cum. examples 5440, speed 3126.98 words/sec, time elapsed 41.67 sec
epoch 1, iter 180, avg. loss 138.28, avg. ppl 152.67 cum. examples 5760, speed 3605.71 words/sec, time elapsed 44.11 sec
epoch 1, iter 190, avg. loss 136.78, avg. ppl 164.81 cum. examples 6080, speed 3145.49 words/sec, time elapsed 46.84 sec
epoch 1, iter 200, avg. loss 128.61, avg. ppl 140.94 cum. examples 6400, speed 3063.76 words/sec, time elapsed 49.55 sec
epoch 1, iter 200, cum. loss 148.46, cum. ppl 283.50 cum. examples 6400
begin validation ...
validation: iter 200, dev. ppl 147.778663
save currently the best model to [model.bin]
save model parameters to [model.bin]
epoch 1, iter 210, avg. loss 123.52, avg. ppl 151.37 cum. examples 320, speed 611.10 words/sec, time elapsed 62.44 sec
epoch 1, iter 220, avg. loss 133.41, avg. ppl 138.75 cum. examples 640, speed 3567.38 words/sec, time elapsed 64.86 sec
epoch 1, iter 230, avg. loss 122.54, avg. ppl 117.69 cum. examples 960, speed 3661.16 words/sec, time elapsed 67.11 sec
epoch 1, iter 240, avg. loss 127.17, avg. ppl 117.95 cum. examples 1280, speed 3409.25 words/sec, time elapsed 69.61 sec
epoch 1, iter 250, avg. loss 127.65, avg. ppl 117.68 cum. examples 1600, speed 3604.89 words/sec, time elapsed 71.99 sec
epoch 1, iter 260, avg. loss 127.04, avg. ppl 103.95 cum. examples 1920, speed 3659.14 words/sec, time elapsed 74.38 sec
epoch 1, iter 270, avg. loss 114.66, avg. ppl 103.10 cum. examples 2240, speed 3619.44 words/sec, time elapsed 76.57 sec
epoch 1, iter 280, avg. loss 121.52, avg. ppl 106.82 cum. examples 2560, speed 3447.74 words/sec, time elapsed 78.98 sec
epoch 1, iter 290, avg. loss 130.30, avg. ppl 126.37 cum. examples 2880, speed 3217.63 words/sec, time elapsed 81.66 sec
epoch 1, iter 300, avg. loss 121.74, avg. ppl 104.64 cum. examples 3200, speed 3552.05 words/sec, time elapsed 84.02 sec
epoch 1, iter 310, avg. loss 121.71, avg. ppl 97.12 cum. examples 3520, speed 3548.11 words/sec, time elapsed 86.42 sec
epoch 1, iter 320, avg. loss 118.97, avg. ppl 94.18 cum. examples 3840, speed 3193.17 words/sec, time elapsed 89.04 sec
epoch 1, iter 330, avg. loss 122.50, avg. ppl 102.30 cum. examples 4160, speed 3618.79 words/sec, time elapsed 91.38 sec
epoch 1, iter 340, avg. loss 105.34, avg. ppl 86.40 cum. examples 4480, speed 3419.44 words/sec, time elapsed 93.59 sec
epoch 1, iter 350, avg. loss 113.68, avg. ppl 89.26 cum. examples 4800, speed 3699.33 words/sec, time elapsed 95.78 sec
epoch 1, iter 360, avg. loss 113.90, avg. ppl 87.18 cum. examples 5120, speed 3131.89 words/sec, time elapsed 98.39 sec
epoch 1, iter 370, avg. loss 114.18, avg. ppl 90.81 cum. examples 5440, speed 3353.33 words/sec, time elapsed 100.80 sec
epoch 1, iter 380, avg. loss 115.62, avg. ppl 97.82 cum. examples 5760, speed 3337.17 words/sec, time elapsed 103.22 sec
epoch 1, iter 390, avg. loss 118.02, avg. ppl 83.12 cum. examples 6080, speed 3232.10 words/sec, time elapsed 105.87 sec
epoch 1, iter 400, avg. loss 119.33, avg. ppl 83.71 cum. examples 6400, speed 3315.31 words/sec, time elapsed 108.47 sec
epoch 1, iter 400, cum. loss 120.64, cum. ppl 103.67 cum. examples 6400
begin validation ...
validation: iter 400, dev. ppl 90.612784
save currently the best model to [model.bin]
save model parameters to [model.bin]
epoch 1, iter 410, avg. loss 110.41, avg. ppl 72.96 cum. examples 320, speed 662.00 words/sec, time elapsed 120.91 sec
epoch 1, iter 420, avg. loss 112.96, avg. ppl 70.30 cum. examples 640, speed 3849.72 words/sec, time elapsed 123.12 sec
epoch 1, iter 430, avg. loss 115.10, avg. ppl 86.56 cum. examples 960, speed 3633.08 words/sec, time elapsed 125.39 sec
epoch 1, iter 440, avg. loss 113.12, avg. ppl 76.65 cum. examples 1280, speed 3325.65 words/sec, time elapsed 127.90 sec
epoch 1, iter 450, avg. loss 108.21, avg. ppl 73.11 cum. examples 1600, speed 3151.94 words/sec, time elapsed 130.46 sec
epoch 1, iter 460, avg. loss 115.15, avg. ppl 81.26 cum. examples 1920, speed 3276.64 words/sec, time elapsed 133.02 sec
epoch 2, iter 470, avg. loss 111.16, avg. ppl 62.87 cum. examples 2215, speed 3259.21 words/sec, time elapsed 135.45 sec
epoch 2, iter 480, avg. loss 105.69, avg. ppl 54.04 cum. examples 2535, speed 3536.89 words/sec, time elapsed 137.84 sec
epoch 2, iter 490, avg. loss 103.79, avg. ppl 55.70 cum. examples 2855, speed 3279.07 words/sec, time elapsed 140.36 sec
epoch 2, iter 500, avg. loss 100.36, avg. ppl 53.49 cum. examples 3175, speed 3154.59 words/sec, time elapsed 142.92 sec
epoch 2, iter 510, avg. loss 102.77, avg. ppl 52.50 cum. examples 3495, speed 3085.01 words/sec, time elapsed 145.61 sec
epoch 2, iter 520, avg. loss 100.52, avg. ppl 54.85 cum. examples 3815, speed 3468.72 words/sec, time elapsed 147.93 sec
epoch 2, iter 530, avg. loss 104.82, avg. ppl 49.40 cum. examples 4135, speed 3213.94 words/sec, time elapsed 150.60 sec
epoch 2, iter 540, avg. loss 103.65, avg. ppl 51.72 cum. examples 4455, speed 3238.37 words/sec, time elapsed 153.20 sec
epoch 2, iter 550, avg. loss 101.71, avg. ppl 47.62 cum. examples 4775, speed 3222.10 words/sec, time elapsed 155.81 sec
epoch 2, iter 560, avg. loss 104.57, avg. ppl 50.79 cum. examples 5095, speed 3571.01 words/sec, time elapsed 158.20 sec
epoch 2, iter 570, avg. loss 105.41, avg. ppl 57.22 cum. examples 5415, speed 3161.41 words/sec, time elapsed 160.84 sec
epoch 2, iter 580, avg. loss 101.27, avg. ppl 53.48 cum. examples 5735, speed 3436.72 words/sec, time elapsed 163.21 sec
epoch 2, iter 590, avg. loss 106.27, avg. ppl 45.13 cum. examples 6055, speed 3354.97 words/sec, time elapsed 165.87 sec
epoch 2, iter 600, avg. loss 105.27, avg. ppl 49.24 cum. examples 6375, speed 3584.17 words/sec, time elapsed 168.28 sec
epoch 2, iter 600, cum. loss 106.59, cum. ppl 58.72 cum. examples 6375
begin validation ...
validation: iter 600, dev. ppl 68.007247
save currently the best model to [model.bin]
save model parameters to [model.bin]
epoch 2, iter 610, avg. loss 105.50, avg. ppl 53.17 cum. examples 320, speed 752.68 words/sec, time elapsed 179.57 sec
epoch 2, iter 620, avg. loss 105.30, avg. ppl 46.87 cum. examples 640, speed 3685.08 words/sec, time elapsed 181.94 sec
epoch 2, iter 630, avg. loss 92.46, avg. ppl 42.67 cum. examples 960, speed 3421.87 words/sec, time elapsed 184.25 sec
epoch 2, iter 640, avg. loss 112.83, avg. ppl 54.43 cum. examples 1280, speed 3241.98 words/sec, time elapsed 187.03 sec
epoch 2, iter 650, avg. loss 95.64, avg. ppl 47.41 cum. examples 1600, speed 3188.59 words/sec, time elapsed 189.52 sec
epoch 2, iter 660, avg. loss 98.27, avg. ppl 47.06 cum. examples 1920, speed 2982.75 words/sec, time elapsed 192.26 sec
epoch 2, iter 670, avg. loss 94.47, avg. ppl 45.18 cum. examples 2240, speed 3661.68 words/sec, time elapsed 194.43 sec
epoch 2, iter 680, avg. loss 101.73, avg. ppl 47.25 cum. examples 2560, speed 3393.49 words/sec, time elapsed 196.91 sec
epoch 2, iter 690, avg. loss 103.89, avg. ppl 49.60 cum. examples 2880, speed 3512.13 words/sec, time elapsed 199.34 sec
epoch 2, iter 700, avg. loss 100.59, avg. ppl 41.26 cum. examples 3200, speed 3260.70 words/sec, time elapsed 201.99 sec
epoch 2, iter 710, avg. loss 98.32, avg. ppl 42.33 cum. examples 3520, speed 3577.60 words/sec, time elapsed 204.34 sec
epoch 2, iter 720, avg. loss 101.65, avg. ppl 47.58 cum. examples 3840, speed 3204.33 words/sec, time elapsed 206.97 sec
epoch 2, iter 730, avg. loss 101.70, avg. ppl 47.88 cum. examples 4160, speed 3506.85 words/sec, time elapsed 209.37 sec
epoch 2, iter 740, avg. loss 100.84, avg. ppl 43.94 cum. examples 4480, speed 3613.64 words/sec, time elapsed 211.73 sec
epoch 2, iter 750, avg. loss 94.14, avg. ppl 39.09 cum. examples 4800, speed 3659.08 words/sec, time elapsed 213.97 sec
epoch 2, iter 760, avg. loss 97.16, avg. ppl 40.88 cum. examples 5120, speed 3578.27 words/sec, time elapsed 216.32 sec
epoch 2, iter 770, avg. loss 97.29, avg. ppl 42.15 cum. examples 5440, speed 3325.91 words/sec, time elapsed 218.82 sec
epoch 2, iter 780, avg. loss 95.86, avg. ppl 39.74 cum. examples 5760, speed 3459.66 words/sec, time elapsed 221.23 sec
epoch 2, iter 790, avg. loss 95.83, avg. ppl 42.10 cum. examples 6080, speed 3434.71 words/sec, time elapsed 223.61 sec
epoch 2, iter 800, avg. loss 95.28, avg. ppl 41.46 cum. examples 6400, speed 3589.18 words/sec, time elapsed 225.89 sec
epoch 2, iter 800, cum. loss 99.44, cum. ppl 44.97 cum. examples 6400
begin validation ...
validation: iter 800, dev. ppl 56.025317
save currently the best model to [model.bin]
save model parameters to [model.bin]
epoch 2, iter 810, avg. loss 100.49, avg. ppl 43.39 cum. examples 320, speed 1088.29 words/sec, time elapsed 233.73 sec
epoch 2, iter 820, avg. loss 97.68, avg. ppl 39.36 cum. examples 640, speed 3713.07 words/sec, time elapsed 236.02 sec
epoch 2, iter 830, avg. loss 94.48, avg. ppl 41.74 cum. examples 960, speed 3698.43 words/sec, time elapsed 238.21 sec
epoch 2, iter 840, avg. loss 97.28, avg. ppl 39.51 cum. examples 1280, speed 3516.30 words/sec, time elapsed 240.62 sec
epoch 2, iter 850, avg. loss 93.10, avg. ppl 37.93 cum. examples 1600, speed 3151.72 words/sec, time elapsed 243.22 sec
epoch 2, iter 860, avg. loss 94.12, avg. ppl 39.26 cum. examples 1920, speed 3436.73 words/sec, time elapsed 245.61 sec
epoch 2, iter 870, avg. loss 95.78, avg. ppl 40.06 cum. examples 2240, speed 3266.39 words/sec, time elapsed 248.15 sec
epoch 2, iter 880, avg. loss 92.47, avg. ppl 38.27 cum. examples 2560, speed 3515.38 words/sec, time elapsed 250.46 sec
epoch 2, iter 890, avg. loss 93.24, avg. ppl 38.58 cum. examples 2880, speed 3337.84 words/sec, time elapsed 252.91 sec
epoch 2, iter 900, avg. loss 89.29, avg. ppl 34.77 cum. examples 3200, speed 3485.96 words/sec, time elapsed 255.22 sec
epoch 2, iter 910, avg. loss 96.00, avg. ppl 39.01 cum. examples 3520, speed 3065.42 words/sec, time elapsed 257.95 sec
epoch 2, iter 920, avg. loss 97.72, avg. ppl 39.31 cum. examples 3840, speed 3826.02 words/sec, time elapsed 260.18 sec
epoch 2, iter 930, avg. loss 93.35, avg. ppl 35.22 cum. examples 4135, speed 3456.59 words/sec, time elapsed 262.42 sec
epoch 3, iter 940, avg. loss 83.36, avg. ppl 25.10 cum. examples 4455, speed 3454.72 words/sec, time elapsed 264.81 sec
epoch 3, iter 950, avg. loss 85.11, avg. ppl 25.30 cum. examples 4775, speed 3595.46 words/sec, time elapsed 267.16 sec
epoch 3, iter 960, avg. loss 83.89, avg. ppl 23.36 cum. examples 5095, speed 3466.87 words/sec, time elapsed 269.62 sec
epoch 3, iter 970, avg. loss 80.26, avg. ppl 24.39 cum. examples 5415, speed 3621.62 words/sec, time elapsed 271.84 sec
epoch 3, iter 980, avg. loss 86.29, avg. ppl 22.47 cum. examples 5735, speed 3529.39 words/sec, time elapsed 274.35 sec
epoch 3, iter 990, avg. loss 84.19, avg. ppl 24.22 cum. examples 6055, speed 3454.15 words/sec, time elapsed 276.80 sec
epoch 3, iter 1000, avg. loss 82.85, avg. ppl 24.23 cum. examples 6375, speed 3818.60 words/sec, time elapsed 278.98 sec
epoch 3, iter 1000, cum. loss 91.04, cum. ppl 32.85 cum. examples 6375
begin validation ...
validation: iter 1000, dev. ppl 48.565791
save currently the best model to [model.bin]
save model parameters to [model.bin]
epoch 3, iter 1010, avg. loss 82.63, avg. ppl 21.79 cum. examples 320, speed 757.13 words/sec, time elapsed 290.31 sec
epoch 3, iter 1020, avg. loss 85.43, avg. ppl 25.45 cum. examples 640, speed 3093.07 words/sec, time elapsed 293.04 sec
epoch 3, iter 1030, avg. loss 77.23, avg. ppl 21.10 cum. examples 960, speed 3429.35 words/sec, time elapsed 295.40 sec
epoch 3, iter 1040, avg. loss 81.90, avg. ppl 24.65 cum. examples 1280, speed 3467.60 words/sec, time elapsed 297.76 sec
epoch 3, iter 1050, avg. loss 84.86, avg. ppl 25.21 cum. examples 1600, speed 3225.31 words/sec, time elapsed 300.37 sec
epoch 3, iter 1060, avg. loss 81.29, avg. ppl 23.03 cum. examples 1920, speed 3578.18 words/sec, time elapsed 302.69 sec
epoch 3, iter 1070, avg. loss 83.48, avg. ppl 21.38 cum. examples 2240, speed 3488.30 words/sec, time elapsed 305.19 sec
epoch 3, iter 1080, avg. loss 80.02, avg. ppl 21.68 cum. examples 2560, speed 3297.90 words/sec, time elapsed 307.71 sec
epoch 3, iter 1090, avg. loss 76.45, avg. ppl 20.51 cum. examples 2880, speed 3518.04 words/sec, time elapsed 310.02 sec
epoch 3, iter 1100, avg. loss 86.17, avg. ppl 24.48 cum. examples 3200, speed 3287.31 words/sec, time elapsed 312.64 sec
epoch 3, iter 1110, avg. loss 84.10, avg. ppl 23.95 cum. examples 3520, speed 2652.23 words/sec, time elapsed 315.83 sec
epoch 3, iter 1120, avg. loss 84.06, avg. ppl 25.68 cum. examples 3840, speed 3297.01 words/sec, time elapsed 318.35 sec
epoch 3, iter 1130, avg. loss 74.95, avg. ppl 19.14 cum. examples 4160, speed 3298.22 words/sec, time elapsed 320.81 sec
epoch 3, iter 1140, avg. loss 80.15, avg. ppl 21.86 cum. examples 4480, speed 3433.09 words/sec, time elapsed 323.23 sec
epoch 3, iter 1150, avg. loss 84.70, avg. ppl 23.14 cum. examples 4800, speed 3495.00 words/sec, time elapsed 325.70 sec
epoch 3, iter 1160, avg. loss 82.53, avg. ppl 21.40 cum. examples 5120, speed 3483.57 words/sec, time elapsed 328.18 sec
epoch 3, iter 1170, avg. loss 77.68, avg. ppl 21.50 cum. examples 5440, speed 3442.13 words/sec, time elapsed 330.53 sec
epoch 3, iter 1180, avg. loss 81.22, avg. ppl 22.14 cum. examples 5760, speed 3351.24 words/sec, time elapsed 333.03 sec
epoch 3, iter 1190, avg. loss 80.39, avg. ppl 22.83 cum. examples 6080, speed 3653.35 words/sec, time elapsed 335.29 sec
epoch 3, iter 1200, avg. loss 76.16, avg. ppl 20.60 cum. examples 6400, speed 3633.38 words/sec, time elapsed 337.50 sec
epoch 3, iter 1200, cum. loss 81.27, cum. ppl 22.52 cum. examples 6400
begin validation ...
validation: iter 1200, dev. ppl 43.960026
save currently the best model to [model.bin]
save model parameters to [model.bin]
epoch 3, iter 1210, avg. loss 76.18, avg. ppl 20.84 cum. examples 320, speed 1034.93 words/sec, time elapsed 345.26 sec
epoch 3, iter 1220, avg. loss 79.91, avg. ppl 22.06 cum. examples 640, speed 3762.43 words/sec, time elapsed 347.46 sec
epoch 3, iter 1230, avg. loss 85.08, avg. ppl 24.28 cum. examples 960, speed 3475.90 words/sec, time elapsed 349.91 sec
epoch 3, iter 1240, avg. loss 81.82, avg. ppl 22.69 cum. examples 1280, speed 3581.13 words/sec, time elapsed 352.25 sec
epoch 3, iter 1250, avg. loss 79.61, avg. ppl 18.16 cum. examples 1600, speed 3666.33 words/sec, time elapsed 354.65 sec
epoch 3, iter 1260, avg. loss 82.51, avg. ppl 22.32 cum. examples 1920, speed 3407.61 words/sec, time elapsed 357.15 sec
epoch 3, iter 1270, avg. loss 86.65, avg. ppl 23.51 cum. examples 2240, speed 2957.56 words/sec, time elapsed 360.12 sec
epoch 3, iter 1280, avg. loss 75.98, avg. ppl 19.16 cum. examples 2560, speed 3505.26 words/sec, time elapsed 362.46 sec
epoch 3, iter 1290, avg. loss 76.73, avg. ppl 21.36 cum. examples 2880, speed 3583.01 words/sec, time elapsed 364.70 sec
epoch 3, iter 1300, avg. loss 74.34, avg. ppl 20.54 cum. examples 3200, speed 3283.56 words/sec, time elapsed 367.10 sec
epoch 3, iter 1310, avg. loss 85.55, avg. ppl 23.51 cum. examples 3520, speed 3423.08 words/sec, time elapsed 369.63 sec
epoch 3, iter 1320, avg. loss 81.24, avg. ppl 22.38 cum. examples 3840, speed 3322.70 words/sec, time elapsed 372.15 sec
epoch 3, iter 1330, avg. loss 78.23, avg. ppl 21.27 cum. examples 4160, speed 3570.24 words/sec, time elapsed 374.44 sec
epoch 3, iter 1340, avg. loss 80.73, avg. ppl 22.22 cum. examples 4480, speed 3335.27 words/sec, time elapsed 376.94 sec
epoch 3, iter 1350, avg. loss 79.61, avg. ppl 21.06 cum. examples 4800, speed 3212.69 words/sec, time elapsed 379.54 sec
epoch 3, iter 1360, avg. loss 81.81, avg. ppl 21.98 cum. examples 5120, speed 3227.22 words/sec, time elapsed 382.17 sec
epoch 3, iter 1370, avg. loss 73.16, avg. ppl 19.08 cum. examples 5440, speed 3687.91 words/sec, time elapsed 384.32 sec
epoch 3, iter 1380, avg. loss 85.99, avg. ppl 24.09 cum. examples 5760, speed 2934.57 words/sec, time elapsed 387.27 sec
epoch 3, iter 1390, avg. loss 79.81, avg. ppl 22.89 cum. examples 6080, speed 3552.93 words/sec, time elapsed 389.57 sec
epoch 4, iter 1400, avg. loss 73.30, avg. ppl 15.08 cum. examples 6375, speed 3151.55 words/sec, time elapsed 392.10 sec
epoch 4, iter 1400, cum. loss 79.94, cum. ppl 21.34 cum. examples 6375
begin validation ...
validation: iter 1400, dev. ppl 39.754194
save currently the best model to [model.bin]
save model parameters to [model.bin]
epoch 4, iter 1410, avg. loss 61.59, avg. ppl 11.24 cum. examples 320, speed 900.91 words/sec, time elapsed 401.14 sec
epoch 4, iter 1420, avg. loss 60.33, avg. ppl 10.41 cum. examples 640, speed 3494.53 words/sec, time elapsed 403.49 sec
epoch 4, iter 1430, avg. loss 67.82, avg. ppl 12.71 cum. examples 960, speed 3196.63 words/sec, time elapsed 406.16 sec
epoch 4, iter 1440, avg. loss 70.69, avg. ppl 13.28 cum. examples 1280, speed 3260.78 words/sec, time elapsed 408.85 sec
epoch 4, iter 1450, avg. loss 64.81, avg. ppl 11.85 cum. examples 1600, speed 3301.95 words/sec, time elapsed 411.39 sec
epoch 4, iter 1460, avg. loss 67.60, avg. ppl 14.88 cum. examples 1920, speed 3355.90 words/sec, time elapsed 413.78 sec
epoch 4, iter 1470, avg. loss 66.69, avg. ppl 12.67 cum. examples 2240, speed 3554.25 words/sec, time elapsed 416.14 sec
epoch 4, iter 1480, avg. loss 63.27, avg. ppl 11.72 cum. examples 2560, speed 3372.98 words/sec, time elapsed 418.58 sec
epoch 4, iter 1490, avg. loss 64.28, avg. ppl 12.31 cum. examples 2880, speed 3442.94 words/sec, time elapsed 420.96 sec
epoch 4, iter 1500, avg. loss 68.42, avg. ppl 13.03 cum. examples 3200, speed 3057.22 words/sec, time elapsed 423.75 sec
epoch 4, iter 1510, avg. loss 62.69, avg. ppl 12.00 cum. examples 3520, speed 3388.32 words/sec, time elapsed 426.13 sec
epoch 4, iter 1520, avg. loss 69.00, avg. ppl 13.67 cum. examples 3840, speed 3395.48 words/sec, time elapsed 428.62 sec
epoch 4, iter 1530, avg. loss 64.55, avg. ppl 12.19 cum. examples 4160, speed 3305.35 words/sec, time elapsed 431.12 sec
epoch 4, iter 1540, avg. loss 66.80, avg. ppl 13.32 cum. examples 4480, speed 3439.27 words/sec, time elapsed 433.52 sec
epoch 4, iter 1550, avg. loss 70.18, avg. ppl 14.38 cum. examples 4800, speed 2835.78 words/sec, time elapsed 436.49 sec
epoch 4, iter 1560, avg. loss 61.17, avg. ppl 11.47 cum. examples 5120, speed 3480.90 words/sec, time elapsed 438.79 sec
epoch 4, iter 1570, avg. loss 65.68, avg. ppl 12.85 cum. examples 5440, speed 3633.41 words/sec, time elapsed 441.06 sec
epoch 4, iter 1580, avg. loss 62.49, avg. ppl 12.39 cum. examples 5760, speed 3143.97 words/sec, time elapsed 443.59 sec
epoch 4, iter 1590, avg. loss 64.80, avg. ppl 11.96 cum. examples 6080, speed 3378.11 words/sec, time elapsed 446.06 sec
epoch 4, iter 1600, avg. loss 66.39, avg. ppl 12.32 cum. examples 6400, speed 3461.64 words/sec, time elapsed 448.50 sec
epoch 4, iter 1600, cum. loss 65.46, cum. ppl 12.50 cum. examples 6400
begin validation ...
validation: iter 1600, dev. ppl 40.653098
hit patience 1
hit #1 trial
load previously best model and decay learning rate to 0.000250
restore parameters of the optimizers
epoch 4, iter 1610, avg. loss 67.45, avg. ppl 13.01 cum. examples 320, speed 1513.16 words/sec, time elapsed 454.06 sec
epoch 4, iter 1620, avg. loss 65.98, avg. ppl 11.68 cum. examples 640, speed 3677.66 words/sec, time elapsed 456.40 sec
epoch 4, iter 1630, avg. loss 63.57, avg. ppl 11.81 cum. examples 960, speed 3389.26 words/sec, time elapsed 458.83 sec
epoch 4, iter 1640, avg. loss 61.07, avg. ppl 10.93 cum. examples 1280, speed 3448.87 words/sec, time elapsed 461.20 sec
epoch 4, iter 1650, avg. loss 63.80, avg. ppl 11.17 cum. examples 1600, speed 3410.13 words/sec, time elapsed 463.68 sec
epoch 4, iter 1660, avg. loss 63.38, avg. ppl 11.01 cum. examples 1920, speed 3509.65 words/sec, time elapsed 466.09 sec
epoch 4, iter 1670, avg. loss 64.28, avg. ppl 10.67 cum. examples 2240, speed 3577.89 words/sec, time elapsed 468.52 sec
epoch 4, iter 1680, avg. loss 68.47, avg. ppl 12.14 cum. examples 2560, speed 3181.18 words/sec, time elapsed 471.28 sec
epoch 4, iter 1690, avg. loss 62.56, avg. ppl 11.19 cum. examples 2880, speed 3495.62 words/sec, time elapsed 473.65 sec
epoch 4, iter 1700, avg. loss 63.92, avg. ppl 11.99 cum. examples 3200, speed 3304.03 words/sec, time elapsed 476.14 sec
epoch 4, iter 1710, avg. loss 63.74, avg. ppl 10.83 cum. examples 3520, speed 3729.40 words/sec, time elapsed 478.44 sec
epoch 4, iter 1720, avg. loss 62.70, avg. ppl 11.27 cum. examples 3840, speed 3606.63 words/sec, time elapsed 480.73 sec
epoch 4, iter 1730, avg. loss 62.52, avg. ppl 11.27 cum. examples 4160, speed 3269.91 words/sec, time elapsed 483.26 sec
epoch 4, iter 1740, avg. loss 62.06, avg. ppl 10.66 cum. examples 4480, speed 3641.18 words/sec, time elapsed 485.56 sec
epoch 4, iter 1750, avg. loss 59.57, avg. ppl 10.33 cum. examples 4800, speed 3508.76 words/sec, time elapsed 487.89 sec
epoch 4, iter 1760, avg. loss 66.45, avg. ppl 12.66 cum. examples 5120, speed 3548.24 words/sec, time elapsed 490.25 sec
epoch 4, iter 1770, avg. loss 62.06, avg. ppl 11.80 cum. examples 5440, speed 3547.73 words/sec, time elapsed 492.52 sec
epoch 4, iter 1780, avg. loss 63.33, avg. ppl 10.88 cum. examples 5760, speed 3484.79 words/sec, time elapsed 494.96 sec
epoch 4, iter 1790, avg. loss 64.14, avg. ppl 11.56 cum. examples 6080, speed 3271.26 words/sec, time elapsed 497.52 sec
epoch 4, iter 1800, avg. loss 63.11, avg. ppl 10.90 cum. examples 6400, speed 3568.51 words/sec, time elapsed 499.89 sec
epoch 4, iter 1800, cum. loss 63.71, cum. ppl 11.37 cum. examples 6400
begin validation ...
validation: iter 1800, dev. ppl 39.221488
save currently the best model to [model.bin]
save model parameters to [model.bin]
epoch 4, iter 1810, avg. loss 59.82, avg. ppl 9.99 cum. examples 320, speed 498.78 words/sec, time elapsed 516.57 sec
epoch 4, iter 1820, avg. loss 62.23, avg. ppl 11.08 cum. examples 640, speed 3425.24 words/sec, time elapsed 518.98 sec
epoch 4, iter 1830, avg. loss 64.86, avg. ppl 11.21 cum. examples 960, speed 3549.06 words/sec, time elapsed 521.40 sec
epoch 4, iter 1840, avg. loss 63.48, avg. ppl 11.29 cum. examples 1280, speed 3379.25 words/sec, time elapsed 523.88 sec
epoch 4, iter 1850, avg. loss 64.81, avg. ppl 11.52 cum. examples 1600, speed 3341.61 words/sec, time elapsed 526.42 sec
epoch 4, iter 1860, avg. loss 65.13, avg. ppl 11.54 cum. examples 1895, speed 3419.35 words/sec, time elapsed 528.72 sec
epoch 5, iter 1870, avg. loss 59.27, avg. ppl 9.05 cum. examples 2215, speed 3465.94 words/sec, time elapsed 531.20 sec
epoch 5, iter 1880, avg. loss 54.77, avg. ppl 8.12 cum. examples 2535, speed 3443.44 words/sec, time elapsed 533.64 sec
epoch 5, iter 1890, avg. loss 55.96, avg. ppl 8.24 cum. examples 2855, speed 3345.90 words/sec, time elapsed 536.17 sec
epoch 5, iter 1900, avg. loss 56.36, avg. ppl 8.41 cum. examples 3175, speed 3673.44 words/sec, time elapsed 538.48 sec
epoch 5, iter 1910, avg. loss 53.69, avg. ppl 8.22 cum. examples 3495, speed 3437.53 words/sec, time elapsed 540.85 sec
epoch 5, iter 1920, avg. loss 60.66, avg. ppl 9.61 cum. examples 3815, speed 3108.33 words/sec, time elapsed 543.61 sec
epoch 5, iter 1930, avg. loss 52.01, avg. ppl 7.88 cum. examples 4135, speed 3577.98 words/sec, time elapsed 545.86 sec
epoch 5, iter 1940, avg. loss 55.43, avg. ppl 8.41 cum. examples 4455, speed 2944.52 words/sec, time elapsed 548.69 sec
epoch 5, iter 1950, avg. loss 57.96, avg. ppl 8.69 cum. examples 4775, speed 3618.68 words/sec, time elapsed 551.06 sec
epoch 5, iter 1960, avg. loss 55.90, avg. ppl 8.59 cum. examples 5095, speed 3346.29 words/sec, time elapsed 553.55 sec
epoch 5, iter 1970, avg. loss 59.54, avg. ppl 8.84 cum. examples 5415, speed 3736.18 words/sec, time elapsed 555.89 sec
epoch 5, iter 1980, avg. loss 56.42, avg. ppl 9.01 cum. examples 5735, speed 3196.65 words/sec, time elapsed 558.46 sec
epoch 5, iter 1990, avg. loss 59.22, avg. ppl 9.07 cum. examples 6055, speed 3212.90 words/sec, time elapsed 561.13 sec
epoch 5, iter 2000, avg. loss 56.70, avg. ppl 8.87 cum. examples 6375, speed 3357.64 words/sec, time elapsed 563.61 sec
epoch 5, iter 2000, cum. loss 58.68, cum. ppl 9.30 cum. examples 6375
begin validation ...
validation: iter 2000, dev. ppl 39.746738
hit patience 1
hit #2 trial
load previously best model and decay learning rate to 0.000125
restore parameters of the optimizers
epoch 5, iter 2010, avg. loss 54.21, avg. ppl 8.83 cum. examples 320, speed 1540.41 words/sec, time elapsed 568.78 sec
epoch 5, iter 2020, avg. loss 62.80, avg. ppl 11.03 cum. examples 640, speed 2861.56 words/sec, time elapsed 571.71 sec
epoch 5, iter 2030, avg. loss 59.23, avg. ppl 9.98 cum. examples 960, speed 2969.42 words/sec, time elapsed 574.48 sec
epoch 5, iter 2040, avg. loss 58.05, avg. ppl 9.45 cum. examples 1280, speed 3452.67 words/sec, time elapsed 576.88 sec
epoch 5, iter 2050, avg. loss 54.59, avg. ppl 8.33 cum. examples 1600, speed 3686.33 words/sec, time elapsed 579.11 sec
epoch 5, iter 2060, avg. loss 56.70, avg. ppl 9.02 cum. examples 1920, speed 3602.82 words/sec, time elapsed 581.40 sec
epoch 5, iter 2070, avg. loss 54.20, avg. ppl 8.56 cum. examples 2240, speed 3429.52 words/sec, time elapsed 583.76 sec
epoch 5, iter 2080, avg. loss 57.34, avg. ppl 9.04 cum. examples 2560, speed 3505.95 words/sec, time elapsed 586.14 sec
epoch 5, iter 2090, avg. loss 58.39, avg. ppl 8.92 cum. examples 2880, speed 3604.43 words/sec, time elapsed 588.51 sec
epoch 5, iter 2100, avg. loss 57.66, avg. ppl 9.11 cum. examples 3200, speed 3344.34 words/sec, time elapsed 591.00 sec
epoch 5, iter 2110, avg. loss 57.51, avg. ppl 8.79 cum. examples 3520, speed 3557.06 words/sec, time elapsed 593.38 sec
epoch 5, iter 2120, avg. loss 57.40, avg. ppl 9.52 cum. examples 3840, speed 3137.93 words/sec, time elapsed 595.98 sec
epoch 5, iter 2130, avg. loss 56.97, avg. ppl 9.59 cum. examples 4160, speed 3406.29 words/sec, time elapsed 598.35 sec
epoch 5, iter 2140, avg. loss 56.50, avg. ppl 9.29 cum. examples 4480, speed 3086.15 words/sec, time elapsed 600.98 sec
epoch 5, iter 2150, avg. loss 55.23, avg. ppl 8.17 cum. examples 4800, speed 3634.59 words/sec, time elapsed 603.29 sec
epoch 5, iter 2160, avg. loss 55.85, avg. ppl 8.59 cum. examples 5120, speed 3345.83 words/sec, time elapsed 605.78 sec
epoch 5, iter 2170, avg. loss 62.01, avg. ppl 9.55 cum. examples 5440, speed 2515.35 words/sec, time elapsed 609.27 sec
epoch 5, iter 2180, avg. loss 54.87, avg. ppl 7.93 cum. examples 5760, speed 3562.04 words/sec, time elapsed 611.65 sec
epoch 5, iter 2190, avg. loss 56.21, avg. ppl 8.66 cum. examples 6080, speed 3300.63 words/sec, time elapsed 614.18 sec
epoch 5, iter 2200, avg. loss 63.93, avg. ppl 10.32 cum. examples 6400, speed 3450.96 words/sec, time elapsed 616.72 sec
epoch 5, iter 2200, cum. loss 57.48, cum. ppl 9.11 cum. examples 6400
begin validation ...
validation: iter 2200, dev. ppl 38.932916
save currently the best model to [model.bin]
save model parameters to [model.bin]
epoch 5, iter 2210, avg. loss 57.13, avg. ppl 9.26 cum. examples 320, speed 1041.40 words/sec, time elapsed 624.61 sec
epoch 5, iter 2220, avg. loss 58.89, avg. ppl 9.41 cum. examples 640, speed 3474.65 words/sec, time elapsed 627.03 sec
epoch 5, iter 2230, avg. loss 53.48, avg. ppl 8.11 cum. examples 960, speed 3461.28 words/sec, time elapsed 629.39 sec
epoch 5, iter 2240, avg. loss 54.62, avg. ppl 8.15 cum. examples 1280, speed 3799.43 words/sec, time elapsed 631.58 sec
epoch 5, iter 2250, avg. loss 57.91, avg. ppl 8.94 cum. examples 1600, speed 3506.20 words/sec, time elapsed 633.99 sec
epoch 5, iter 2260, avg. loss 55.61, avg. ppl 8.95 cum. examples 1920, speed 3337.20 words/sec, time elapsed 636.43 sec
epoch 5, iter 2270, avg. loss 57.76, avg. ppl 8.74 cum. examples 2240, speed 3413.67 words/sec, time elapsed 638.93 sec
epoch 5, iter 2280, avg. loss 58.48, avg. ppl 8.65 cum. examples 2560, speed 3594.11 words/sec, time elapsed 641.34 sec
epoch 5, iter 2290, avg. loss 57.69, avg. ppl 8.63 cum. examples 2880, speed 3508.16 words/sec, time elapsed 643.78 sec
epoch 5, iter 2300, avg. loss 57.91, avg. ppl 8.98 cum. examples 3200, speed 3412.15 words/sec, time elapsed 646.25 sec
epoch 5, iter 2310, avg. loss 55.51, avg. ppl 8.55 cum. examples 3520, speed 3477.94 words/sec, time elapsed 648.63 sec
epoch 5, iter 2320, avg. loss 52.90, avg. ppl 8.18 cum. examples 3840, speed 3462.82 words/sec, time elapsed 650.96 sec
epoch 6, iter 2330, avg. loss 51.62, avg. ppl 7.86 cum. examples 4135, speed 3351.56 words/sec, time elapsed 653.16 sec
epoch 6, iter 2340, avg. loss 50.43, avg. ppl 7.29 cum. examples 4455, speed 3445.86 words/sec, time elapsed 655.52 sec
epoch 6, iter 2350, avg. loss 54.95, avg. ppl 7.69 cum. examples 4775, speed 3262.68 words/sec, time elapsed 658.17 sec
epoch 6, iter 2360, avg. loss 52.00, avg. ppl 7.47 cum. examples 5095, speed 3114.97 words/sec, time elapsed 660.82 sec
epoch 6, iter 2370, avg. loss 48.17, avg. ppl 7.07 cum. examples 5415, speed 3610.91 words/sec, time elapsed 663.00 sec
epoch 6, iter 2380, avg. loss 47.60, avg. ppl 6.63 cum. examples 5735, speed 3683.86 words/sec, time elapsed 665.19 sec
epoch 6, iter 2390, avg. loss 48.65, avg. ppl 6.89 cum. examples 6055, speed 3313.65 words/sec, time elapsed 667.62 sec
epoch 6, iter 2400, avg. loss 53.44, avg. ppl 7.19 cum. examples 6375, speed 3445.84 words/sec, time elapsed 670.14 sec
epoch 6, iter 2400, cum. loss 54.25, cum. ppl 8.10 cum. examples 6375
begin validation ...
validation: iter 2400, dev. ppl 39.864504
hit patience 1
hit #3 trial
load previously best model and decay learning rate to 0.000063
restore parameters of the optimizers
epoch 6, iter 2410, avg. loss 54.32, avg. ppl 7.39 cum. examples 320, speed 1706.02 words/sec, time elapsed 675.23 sec
epoch 6, iter 2420, avg. loss 52.16, avg. ppl 7.57 cum. examples 640, speed 3385.87 words/sec, time elapsed 677.67 sec
epoch 6, iter 2430, avg. loss 54.62, avg. ppl 7.97 cum. examples 960, speed 3621.33 words/sec, time elapsed 680.00 sec
epoch 6, iter 2440, avg. loss 54.30, avg. ppl 7.91 cum. examples 1280, speed 3725.81 words/sec, time elapsed 682.25 sec
epoch 6, iter 2450, avg. loss 55.30, avg. ppl 7.93 cum. examples 1600, speed 3278.01 words/sec, time elapsed 684.86 sec
epoch 6, iter 2460, avg. loss 53.23, avg. ppl 7.36 cum. examples 1920, speed 3622.62 words/sec, time elapsed 687.21 sec
epoch 6, iter 2470, avg. loss 53.93, avg. ppl 7.58 cum. examples 2240, speed 3867.54 words/sec, time elapsed 689.42 sec
epoch 6, iter 2480, avg. loss 51.41, avg. ppl 7.49 cum. examples 2560, speed 3671.06 words/sec, time elapsed 691.64 sec
epoch 6, iter 2490, avg. loss 56.42, avg. ppl 8.37 cum. examples 2880, speed 3359.62 words/sec, time elapsed 694.17 sec
epoch 6, iter 2500, avg. loss 60.22, avg. ppl 8.06 cum. examples 3200, speed 3404.49 words/sec, time elapsed 696.88 sec
epoch 6, iter 2510, avg. loss 53.87, avg. ppl 7.83 cum. examples 3520, speed 3504.61 words/sec, time elapsed 699.27 sec
epoch 6, iter 2520, avg. loss 51.89, avg. ppl 7.35 cum. examples 3840, speed 3513.79 words/sec, time elapsed 701.64 sec
epoch 6, iter 2530, avg. loss 56.23, avg. ppl 8.21 cum. examples 4160, speed 3150.21 words/sec, time elapsed 704.36 sec
epoch 6, iter 2540, avg. loss 56.21, avg. ppl 7.86 cum. examples 4480, speed 3479.50 words/sec, time elapsed 706.86 sec
epoch 6, iter 2550, avg. loss 55.27, avg. ppl 8.32 cum. examples 4800, speed 3251.76 words/sec, time elapsed 709.43 sec
epoch 6, iter 2560, avg. loss 49.21, avg. ppl 7.37 cum. examples 5120, speed 3213.87 words/sec, time elapsed 711.88 sec
epoch 6, iter 2570, avg. loss 57.96, avg. ppl 8.39 cum. examples 5440, speed 3209.96 words/sec, time elapsed 714.60 sec
epoch 6, iter 2580, avg. loss 55.97, avg. ppl 8.17 cum. examples 5760, speed 2930.70 words/sec, time elapsed 717.51 sec
epoch 6, iter 2590, avg. loss 55.65, avg. ppl 8.57 cum. examples 6080, speed 3439.64 words/sec, time elapsed 719.92 sec
epoch 6, iter 2600, avg. loss 56.94, avg. ppl 8.02 cum. examples 6400, speed 3183.89 words/sec, time elapsed 722.67 sec
epoch 6, iter 2600, cum. loss 54.76, cum. ppl 7.88 cum. examples 6400
begin validation ...
validation: iter 2600, dev. ppl 39.326515
hit patience 1
hit #4 trial
load previously best model and decay learning rate to 0.000031
restore parameters of the optimizers
epoch 6, iter 2610, avg. loss 54.43, avg. ppl 7.84 cum. examples 320, speed 1602.43 words/sec, time elapsed 727.95 sec
epoch 6, iter 2620, avg. loss 53.13, avg. ppl 8.22 cum. examples 640, speed 3663.55 words/sec, time elapsed 730.15 sec
epoch 6, iter 2630, avg. loss 51.74, avg. ppl 7.62 cum. examples 960, speed 3415.57 words/sec, time elapsed 732.54 sec
epoch 6, iter 2640, avg. loss 51.65, avg. ppl 7.83 cum. examples 1280, speed 3377.21 words/sec, time elapsed 734.92 sec
epoch 6, iter 2650, avg. loss 55.54, avg. ppl 7.90 cum. examples 1600, speed 3466.74 words/sec, time elapsed 737.40 sec
epoch 6, iter 2660, avg. loss 51.94, avg. ppl 7.64 cum. examples 1920, speed 3482.84 words/sec, time elapsed 739.75 sec
epoch 6, iter 2670, avg. loss 52.56, avg. ppl 7.34 cum. examples 2240, speed 3691.97 words/sec, time elapsed 742.03 sec
epoch 6, iter 2680, avg. loss 52.12, avg. ppl 7.83 cum. examples 2560, speed 3327.74 words/sec, time elapsed 744.47 sec
epoch 6, iter 2690, avg. loss 54.44, avg. ppl 8.38 cum. examples 2880, speed 3245.87 words/sec, time elapsed 746.99 sec
epoch 6, iter 2700, avg. loss 49.11, avg. ppl 7.20 cum. examples 3200, speed 3536.13 words/sec, time elapsed 749.24 sec
epoch 6, iter 2710, avg. loss 50.91, avg. ppl 7.14 cum. examples 3520, speed 3356.57 words/sec, time elapsed 751.71 sec
epoch 6, iter 2720, avg. loss 49.96, avg. ppl 7.69 cum. examples 3840, speed 3278.96 words/sec, time elapsed 754.10 sec
epoch 6, iter 2730, avg. loss 52.13, avg. ppl 7.61 cum. examples 4160, speed 3431.57 words/sec, time elapsed 756.50 sec
epoch 6, iter 2740, avg. loss 53.77, avg. ppl 8.05 cum. examples 4480, speed 3486.42 words/sec, time elapsed 758.86 sec
epoch 6, iter 2750, avg. loss 56.37, avg. ppl 8.35 cum. examples 4800, speed 3013.06 words/sec, time elapsed 761.68 sec
epoch 6, iter 2760, avg. loss 51.76, avg. ppl 7.67 cum. examples 5120, speed 3641.90 words/sec, time elapsed 763.92 sec
epoch 6, iter 2770, avg. loss 56.86, avg. ppl 8.23 cum. examples 5440, speed 3549.11 words/sec, time elapsed 766.35 sec
epoch 6, iter 2780, avg. loss 55.08, avg. ppl 7.79 cum. examples 5760, speed 3045.39 words/sec, time elapsed 769.17 sec
epoch 6, iter 2790, avg. loss 54.59, avg. ppl 7.70 cum. examples 6055, speed 3288.60 words/sec, time elapsed 771.57 sec
epoch 7, iter 2800, avg. loss 52.48, avg. ppl 7.49 cum. examples 6375, speed 3395.25 words/sec, time elapsed 774.02 sec
epoch 7, iter 2800, cum. loss 53.02, cum. ppl 7.77 cum. examples 6375
begin validation ...
validation: iter 2800, dev. ppl 39.303794
hit patience 1
hit #5 trial
early stop!
Question 1.8 (3 points)
Run the following to check your model. The BLEU score should be larger than 10.


[ ]
test(NMT, Vocab, UTILS_DIR)
load test source sentences from [/content/COSE461_a4_utils/chr_en_data/test.chr]
load test target sentences from [/content/COSE461_a4_utils/chr_en_data/test.en]
load model from model.bin
Decoding: 100%|██████████| 1000/1000 [00:43<00:00, 23.17it/s]
Corpus BLEU: 12.479447683724725
Question 1.9 [written] (4 points)
In class, we learned about dot product attention, multiplicative attention, and additive attention. As a reminder, dot product attention is et,i=s⊤thi, multiplicative attention is et,i=s⊤tWhi, and additive attention is et,i=v⊤tanh(W1hi+W2st).

Explain one advantage and one disadvantage of dot product attention compared to multiplicative attention. Explain one advantage and one disadvantage of additive attention compared to multiplicative attention.

Dot product attention has the advantage of computational efficiency as it directly computes the similarity between the source and target hidden states without the need for an extra weight matrix. This makes it faster and more memory-efficient compared to multiplicative attention. However, a drawback of dot product attention is its limited capacity to capture complex relationships between the source and target sequences. Without a learnable weight matrix, dot product attention may struggle to model intricate attention patterns and could lead to suboptimal performance in tasks that require capturing subtle dependencies.

Additive attention, on the other hand, offers the advantage of flexibility in capturing non-linear relationships. By applying a tanh activation function on a linear combination of the source and target hidden states, additive attention can model complex interactions and capture nuanced attention patterns. This is particularly beneficial in tasks involving long-range dependencies or complex alignment patterns. However, a downside of additive attention is its increased computational complexity and parameterization. The additional weight matrices and non-linear activation function introduce more parameters and computational overhead, making training and inference slower, especially in large-scale models or resource-constrained environments.

-Dot product attention is space and computationally more efficient than multiplicative attention because it can be realized with very highly optimized matrix multiplication subroutines used in deep learning libraries. Its effectiveness makes it well-suited for big models and real-time situations. However, the disadvantage is dot product attention has a limitation that the query and key vectors should have the same dimension. Thus, the dot products grow very large if not scaled correctly for the high-dimensional vectors, leading to small gradients and potentially unstable training.

-Additive attention can denote more complex key-query relationships as it uses a feedforward neural network with non-linear activation and the query vector and key vector do not require the same length. However, additive attention is computationally more costly and slower in practice compared to multiplicative attention because it cannot fully utilize optimized matrix multiplication and has a higher number of parameters and operations

Congratulations on finishing Assignment 4.
Submission Instructions
Click File -> Save or 파일-> 저장 to save.
Run the code cell below. It requires to mount your Google Drive to VM, so authorize as instructed.
.html file will be automatically downloaded (allow the permission to download file if asked). Submit your .html file on Blackboard.

[ ]
from google.colab import drive, files
from requests import get
from socket import gethostname, gethostbyname

drive.mount('/mnt/')
ip = gethostbyname(gethostname()) # 172.28.0.12
filename = get(f"http://{ip}:9000/api/sessions").json()[0]["name"]

filepath = f'/mnt/My Drive/Colab Notebooks/{filename}'
output_file = f'/mnt/My Drive/Colab Notebooks/Assignment3.html'

!jupyter nbconvert '{filepath}' --to html --output '{output_file}'
!cp '{output_file}' '/content/'
files.download('/content/Assignment3.html')


[ ]

Start coding or generate with AI.
Colab paid products - Cancel contracts here
